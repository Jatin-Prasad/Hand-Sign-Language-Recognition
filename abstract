Hand sign language recognition technology has emerged as a crucial tool for enabling communication between hearing and speech-impaired individuals and the general population. This project aims to develop a real-time hand sign language recognition system using OpenCV, MediaPipe, and machine learning. The system focuses on detecting and recognizing static hand gestures and translating them into corresponding alphabetical or symbolic outputs. This abstract provides an overview of the project methodology, key findings, and implications.
The project begins with the collection of hand gesture data using a webcam. Images of hand signs are captured and stored in a structured dataset directory, ensuring variability in hand orientations, lighting conditions, and background noise. Data preprocessing techniques, including hand landmark detection and normalization using MediaPipe, are applied to extract key features from hand gestures. Each gesture is labeled with a corresponding class, facilitating supervised learning during model training.
A machine learning model is trained using the preprocessed dataset. The extracted features are used to build a robust classifier capable of differentiating between multiple hand gestures. The system performs real-time hand gesture detection and recognition, utilizing MediaPipe for hand tracking and OpenCV for visual feedback. The trained model predicts the meaning of detected gestures and displays the output seamlessly on the webcam feed. The system demonstrates high accuracy and efficiency, highlighting its potential for assistive technologies and educational tools.
